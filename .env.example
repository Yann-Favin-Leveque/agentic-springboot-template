# AgenticAI Environment Configuration
# Copy this to .env and fill in your actual values
# spring-dotenv auto-loads this file on startup

# ===========================================
# PROVIDER FILTER (optional)
# ===========================================
# Comma-separated list of providers to load (lowercase)
# Options: openai, azure-openai, azure-anthropic
ENABLED_PROVIDERS=openai,azure-openai,azure-anthropic

# ===========================================
# LLM INSTANCES (JSON array)
# ===========================================
# Supported providers: openai, azure-openai, azure-anthropic
# NOTE: For .env file, put JSON on a SINGLE LINE (no line breaks)

LLM_INSTANCES=[{"id":"openai-main","url":"https://api.openai.com","key":"YOUR_OPENAI_API_KEY","models":"gpt-4o,gpt-4o-mini,text-embedding-3-small,dall-e-3","provider":"openai","enabled":true},{"id":"azure-example","url":"https://YOUR-RESOURCE.openai.azure.com","key":"YOUR_AZURE_KEY","models":"gpt-4o","provider":"azure-openai","apiVersion":"2024-08-01-preview","enabled":false},{"id":"azure-anthropic-example","url":"https://YOUR-RESOURCE.services.ai.azure.com","key":"YOUR_AZURE_ANTHROPIC_KEY","models":"claude-sonnet-4-5,claude-haiku-4-5","provider":"azure-anthropic","apiVersion":"2023-06-01","enabled":false}]

# ===========================================
# RATE LIMITING & RETRY
# ===========================================
CONCURRENT_STREAM_LIMIT_PER_INSTANCE=15
LLM_MAX_RETRIES=3
LLM_DEFAULT_RESPONSE_TIMEOUT=120000

# ===========================================
# ASYNC THREAD POOL
# ===========================================
ASYNC_POOL_SIZE=30

# ===========================================
# LOGGING
# ===========================================
# Development
LOGGING_LEVEL_IO_GITHUB_YANNFAVINLEVEQUE_AGENTIC=DEBUG
LOGGING_LEVEL_IO_GITHUB_SASHIRESTELA_CLEVERCLIENT=ERROR
LOGGING_LEVEL_ORG_SPRINGFRAMEWORK=INFO

# Production (uncomment)
# LOGGING_LEVEL_IO_GITHUB_YANNFAVINLEVEQUE_AGENTIC=INFO
